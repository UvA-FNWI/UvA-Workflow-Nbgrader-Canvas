{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nbgrader.apps import NbGraderAPI\n",
    "from traitlets.config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import nbgrader\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from canvasapi import Canvas\n",
    "from IPython.display import Javascript, Markdown, display\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive, widgets, Button, Layout\n",
    "from tqdm import tqdm, tqdm_notebook  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def assign(assignment_id):\n",
    "    !nbgrader update {'source/'+assignment_id+'/'+assignment_id+\".ipynb\"}\n",
    "    !nbgrader assign {assignment_id} --create --force --IncludeHeaderFooter.header=source/header.ipynb --log-level='INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def download_files(assignment_id, course):\n",
    "    directory = 'downloaded/%s/archive/' % assignment_id\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get sis id's from students\n",
    "    student_dict = get_student_ids(course)\n",
    "\n",
    "    # Get the Canvas assignment id\n",
    "    assignment = get_assignment_obj(course, assignment_id)\n",
    "    \n",
    "    for submission in tqdm_notebook(assignment.get_submissions()):\n",
    "        # Check if submission has attachments\n",
    "        if 'attachments' not in submission.attributes:\n",
    "            continue\n",
    "        # Download file and give correct name\n",
    "        student_id = student_dict[submission.user_id]\n",
    "        attachment = submission.attributes[\"attachments\"][0]\n",
    "        filename = str(student_id) + \"_\" + assignment_id + \".ipynb\"\n",
    "        urllib.request.urlretrieve(attachment['url'], directory + filename)\n",
    "        # Clear all notebooks of output to save memory\n",
    "        !nbstripout {directory + filename}\n",
    "    # Move the download files to submission folder\n",
    "    !nbgrader zip_collect {assignment_id} --force --log-level='INFO'\n",
    "\n",
    "    # Delete folders which aren't necessary\n",
    "    shutil.rmtree('downloaded/%s/archive/' % assignment_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def update_db(b):\n",
    "    # Check which students are already in nbgrader database\n",
    "    students_already_in_db = [\n",
    "        student.id for student in nbgrader_api.gradebook.students\n",
    "    ]\n",
    "\n",
    "    for student in tqdm_notebook(course.get_users(enrollment_type=['student'])):\n",
    "        first_name, last_name = student.name.split(' ', 1)\n",
    "        # Add students that are not yet in nbgrader database\n",
    "        if student.sis_user_id not in students_already_in_db:\n",
    "            nbgrader_api.gradebook.add_student(\n",
    "                str(student.sis_user_id),\n",
    "                first_name=first_name,\n",
    "                last_name=last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def autograde(assignment_id):\n",
    "    !nbgrader autograde {assignment_id} --create --force --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def plagiatcheck(assignment_id):\n",
    "    !jupyter nbconvert --to script downloaded/{assignment_id}/extracted/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/pyfiles / --log-level WARN\n",
    "    !jupyter nbconvert --to script release/{assignment_id}/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/base / --log-level WARN\n",
    "    shutil.rmtree('downloaded/%s/extracted/' % assignment_id)\n",
    "    directory = \"plagiaatcheck/%s/pyfiles/\" % assignment_id\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            targetfilename = file[:-3] + \"py\"\n",
    "            if targetfilename in os.listdir(directory):\n",
    "                os.remove(directory + targetfilename)\n",
    "            os.rename(directory + file, directory + targetfilename)\n",
    "    if not sys.platform.startswith('win'):\n",
    "        !compare50 plagiaatcheck/{assignment_id}/pyfiles/* -d plagiaatcheck/{assignment_id}/base/\n",
    "        #!compare50 pyfiles/* -d base/\n",
    "    else:\n",
    "        print(\"Oeps, voor compare50 heb je Linux of Mac nodig.\")\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<a class=\"btn btn-primary\" style=\"margin-top: 10px; text-decoration: none;\" href=\"plagiaatcheck/%s/\" target=\"_blank\">Open map met plagiaatresultaten</a>'\n",
    "            % assignment_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_feedback(student_id, assignment_id):\n",
    "    \"\"\"Given a student_id and assignment_id, creates a feedback file without the Hidden Tests\"\"\"\n",
    "    directory = 'feedback/%s/%s/' % (student_id, assignment_id)\n",
    "    soup = str(\n",
    "        BeautifulSoup(\n",
    "            open(\"%s%s.html\" % (directory, assignment_id), encoding='utf-8'),\n",
    "            \"html.parser\"))\n",
    "    css, html = soup.split('</head>', 1)\n",
    "    html = re.sub(\n",
    "        r'(<div class=\"output_subarea output_text output_error\">\\n<pre>\\n)(?:(?!<\\/div>)[\\w\\W])*(<span class=\"ansi-red-intense-fg ansi-bold\">[\\w\\W]*?<\\/pre>)',\n",
    "        r'\\1\\2', html)\n",
    "    html = re.sub(\n",
    "        r'<span class=\"c1\">### BEGIN HIDDEN TESTS<\\/span>[\\w\\W]*?<span class=\"c1\">### END HIDDEN TESTS<\\/span>',\n",
    "        '', html)\n",
    "    soup = css + '</head>' + html\n",
    "    targetdirectory = 'canvasfeedback/%s/%s/' % (student_id, assignment_id)\n",
    "    if not os.path.exists(targetdirectory):\n",
    "        os.makedirs(targetdirectory)\n",
    "    filename = \"%s%s.html\" % (targetdirectory, assignment_id)\n",
    "    Html_file = open(filename, \"w\", encoding=\"utf8\")\n",
    "    Html_file.write(soup)\n",
    "    Html_file.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def calculate_grade(score, min_grade, max_score):\n",
    "    \"\"\"Calculate grade for an assignment\"\"\"\n",
    "    return max(\n",
    "        1, min(\n",
    "            round(min_grade + (10 - min_grade) * score / max_score, 1), 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_grades_per_assignment(assignment_name, gradedict):\n",
    "    canvasdf = pd.DataFrame(\n",
    "        nbgrader_api.gradebook.submission_dicts(assignment_name)).set_index(\n",
    "            'student')\n",
    "    if assignment_name in gradedict.keys():\n",
    "        max_score = gradedict[assignment_name][\"max_score\"]\n",
    "        min_grade = gradedict[assignment_name][\"min_grade\"]\n",
    "    else:\n",
    "        max_score = canvasdf['max_score'].max()\n",
    "        min_grade = 0\n",
    "\n",
    "    canvasdf['grade'] = canvasdf['score'].apply(\n",
    "        lambda row: calculate_grade(row, min_grade, max_score))\n",
    "    canvasdf = canvasdf.pivot_table(\n",
    "        values='grade', index='student', columns='name', aggfunc='first')\n",
    "    return canvasdf\n",
    "\n",
    "\n",
    "def total_df(gradedict):\n",
    "\n",
    "    canvasdf = pd.concat([\n",
    "        create_grades_per_assignment(x, gradedict)\n",
    "        for x in graded_submissions()\n",
    "    ],axis=1)\n",
    "    return canvasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def color_grades(row):\n",
    "    if row['interval'].right <= 5.5:\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'g'\n",
    "\n",
    "\n",
    "def visualize_grades(assignment_id, gradedict):\n",
    "    \"\"\"Creates a plot of the grades from a specific assignment\"\"\"\n",
    "    grades = create_grades_per_assignment(assignment_id,\n",
    "                                          gradedict)[assignment_id]\n",
    "    # ignore grades equal to 1.0\n",
    "    grades = grades.where(grades >= 1.0).dropna()\n",
    "    print(\"The mean grade is {:.1f}\".format(grades.mean()))\n",
    "    print(\"The median grade is {}\".format(grades.median()))\n",
    "    print(\"Maximum van Cohen-Schotanus is {:.1f}\".format(\n",
    "        grades.nlargest(max(5, int(len(grades) * 0.05))).mean()))\n",
    "    print(\"Het percentage onvoldoendes is {:.1f}%. \".format(\n",
    "        100 * sum(grades < 5.5) / len(grades)))\n",
    "    if 100 * sum(grades < 5.5) / len(grades) > 30:\n",
    "        print(\n",
    "            \"Het percentage onvoldoendes is te hoog, voor meer informatie kijk op: {}\"\n",
    "            .format(\n",
    "                \"http://toetsing.uva.nl/toetscyclus/analyseren/tentamenanalyse/tentamenanalyse.html#anker-percentage-geslaagde-studenten\"\n",
    "            ))\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    bins = np.arange(1, 10, 0.5)\n",
    "    interval = [pd.Interval(x, x + 0.5, closed='left') for x in bins]\n",
    "    interval[-1] = pd.Interval(left=9.5, right=10.001, closed='left')\n",
    "    interval = pd.IntervalIndex(interval)\n",
    "    new_grades = grades.groupby([pd.cut(grades, interval)]).size()\n",
    "    test_grades = pd.DataFrame(new_grades)\n",
    "    test_grades.columns = [\"Test\"]\n",
    "    test_grades = test_grades.reset_index()\n",
    "    test_grades.columns = [\"interval\", \"Test\"]\n",
    "    test_grades['color'] = test_grades.apply(color_grades, axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(1, 10)\n",
    "    ax.xaxis.set_ticks(range(1, 11))\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.yaxis.set_ticks([])\n",
    "    ax.bar(\n",
    "        bins, new_grades, width=0.5, align=\"edge\", color=test_grades['color'])\n",
    "    sns.kdeplot(grades, ax=ax2, clip=(1, 10))\n",
    "\n",
    "\n",
    "def p_value(df):\n",
    "    return df.groupby(\n",
    "        'question_name', sort=False)['final_score'].mean() / df.groupby(\n",
    "            'question_name', sort=False)['max_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_results_per_question():\n",
    "    q = '''\n",
    "        SELECT\n",
    "            submitted_assignment.student_id,\n",
    "            grade_cell.name AS question_name,\n",
    "            grade_cell.max_score,\n",
    "            grade.needs_manual_grade AS needs_grading,\n",
    "            grade.auto_score,\n",
    "            grade.manual_score,\n",
    "            grade.extra_credit,\n",
    "            assignment.name AS assignment\n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql_query(q, 'sqlite:///gradebook.db')\n",
    "\n",
    "    df['final_score'] = np.where(\n",
    "        ~pd.isnull(df['manual_score']), df['manual_score'],\n",
    "        df['auto_score']) + df['extra_credit'].fillna(0)\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_visualizations(assignment_id):\n",
    "    df = create_results_per_question()\n",
    "    df = df.loc[df['assignment'] == assignment_id]\n",
    "    p_df = p_value(df)\n",
    "    rir_df = create_rir(df)\n",
    "    combined_df = pd.concat([p_df, rir_df], axis=1)\n",
    "    combined_df = combined_df.reindex(list(p_df.index))\n",
    "    combined_df = combined_df.reset_index()\n",
    "    combined_df.columns = [\"Question\", \"P value\", \"Rir value\", \"positive\"]\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 7), sharey=True)\n",
    "    plt.suptitle('P value and Rir value per question')\n",
    "    sns.barplot(\n",
    "        x=\"P value\", y=\"Question\", data=combined_df, color='b',\n",
    "        ax=axes[0]).set_xlim(0, 1.0)\n",
    "    sns.barplot(\n",
    "        x=\"Rir value\",\n",
    "        y=\"Question\",\n",
    "        data=combined_df,\n",
    "        ax=axes[1],\n",
    "        palette=combined_df[\"positive\"]).set_xlim(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['Rir-waarde'] <= 0:\n",
    "        return 'r'\n",
    "    elif row['Rir-waarde'] <= 0.25:\n",
    "        return 'y'\n",
    "    else:\n",
    "        return 'g'\n",
    "\n",
    "\n",
    "def create_rir(df):\n",
    "    testdict = {}\n",
    "\n",
    "    if len(df[\"student_id\"].unique()) <= 50:\n",
    "        print(\"Norm of 50 students not reached to be meaningful\")\n",
    "\n",
    "    df[\"total_score_item\"] = df[\"extra_credit\"] + df[\"auto_score\"] + df[\n",
    "        \"manual_score\"]\n",
    "    df['student_score-item'] = df['total_score_item'].groupby(\n",
    "        df['student_id']).transform('sum') - df['total_score_item']\n",
    "    for question in sorted(set(df[\"question_name\"].values)):\n",
    "        temp_df = df.loc[df['question_name'] == question]\n",
    "        testdict[question] = temp_df[[\n",
    "            \"total_score_item\", \"student_score-item\"\n",
    "        ]].corr().iloc[1, 0]\n",
    "    testdf = pd.DataFrame.from_dict(\n",
    "        testdict, orient='index', columns=[\"Rir-waarde\"])\n",
    "    testdf['positive'] = testdf.apply(f, axis=1)\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def upload_to_canvas(assignment_name, message, feedback=False):\n",
    "    print(feedback,assignment_name)\n",
    "    if feedback:\n",
    "        !nbgrader feedback --quiet --force --assignment={assignment_name}\n",
    "        \n",
    "    # Haal de laatste cijfers uit gradebook\n",
    "    canvasdf = total_df(gradedict)\n",
    "    student_dict = get_student_ids(course)\n",
    "    \n",
    "    assignment = get_assignment_obj(course, assignment_name)\n",
    "    # loop over alle submissions voor een assignment, alleen als er attachments zijn\n",
    "    for submission in tqdm_notebook(\n",
    "            assignment.get_submissions(), desc='Submissions', leave=False):\n",
    "        try:\n",
    "            student_id = student_dict[submission.user_id]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if int(student_id) not in list(canvasdf.index.values):\n",
    "            continue\n",
    "        grade = canvasdf.at[int(student_id), assignment_name]\n",
    "        if np.isnan(grade):\n",
    "            continue\n",
    "        # alleen de cijfers veranderen als die op canvas lager zijn of niet bestaan\n",
    "        if submission.attributes['score'] == None:\n",
    "            pass\n",
    "        elif submission.attributes['score'] == grade or submission.attributes['score'] == 0:\n",
    "            continue\n",
    "        print(grade, student_id)\n",
    "        if feedback:\n",
    "            feedbackfile = create_feedback(student_id,\n",
    "                                       assignment_name)\n",
    "            submission.upload_comment(feedbackfile)\n",
    "        submission.edit(submission={'posted_grade': str(grade)}, comment={'text_comment':message})\n",
    "        \n",
    "    # feedbackfile verwijderen, om ruimte te besparen.\n",
    "    if 'canvasfeedback' in os.listdir():\n",
    "        shutil.rmtree('canvasfeedback/', ignore_errors=True)\n",
    "    if 'feedback' in os.listdir():\n",
    "        shutil.rmtree('feedback/', ignore_errors=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_button = Button(\n",
    "    description=\"Update the students in the database\",\n",
    "    layout=Layout(width='300px'))\n",
    "db_button.on_click(update_db)\n",
    "\n",
    "interact_assign = interact_manual.options(\n",
    "    manual_name=\"Assign de assignment in de database\")\n",
    "\n",
    "\n",
    "canvas_button = interact_manual.options(\n",
    "    manual_name=\"Cijfers naar Canvas jwz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overview(df, sequence):\n",
    "    df = df.fillna(0)\n",
    "    testlist = []\n",
    "    l = [x for x in sequence if x in df.columns]\n",
    "\n",
    "    for n, c in enumerate(l):\n",
    "\n",
    "        kolommen_assignments = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"AssignmentWeek\")])\n",
    "        kolommen_deeltoets = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"Deeltoets\")])\n",
    "        temp = df[df[c] > 0]\n",
    "        if kolommen_deeltoets == set():\n",
    "            voldoende_deeltoets = pd.Series(\n",
    "                [True for x in range(len(df.index))], index=df.index)\n",
    "        else:\n",
    "            voldoende_deeltoets = temp[kolommen_deeltoets].mean(axis=1) >= 5.5\n",
    "        voldoende_assignments = temp[kolommen_assignments].mean(axis=1) >= 5.5\n",
    "        testlist.append(\n",
    "            [c] + [len(df) - len(temp)] +\n",
    "            [(x & y).sum()\n",
    "             for x in [~voldoende_deeltoets, voldoende_deeltoets]\n",
    "             for y in [~voldoende_assignments, voldoende_assignments]])\n",
    "\n",
    "    testdf = pd.DataFrame(\n",
    "        testlist,\n",
    "        columns=[\n",
    "            \"Assignment Name\", \"Heeft niet meegedaan aan deze opdracht\",\n",
    "            \"Onvoldoende voor beide onderdelen\", \"Onvoldoende voor deeltoets\",\n",
    "            \"Onvoldoende voor assignments\", \"Voldoende voor beide onderdelen\"\n",
    "        ]).set_index(\"Assignment Name\")\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_overview(gradedict, sequence):\n",
    "    df = total_df(gradedict)\n",
    "    overviewdf = create_overview(df, sequence)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.suptitle('Overview of the course')\n",
    "    df = df.reindex([x for x in sequence if x in df.columns], axis=1)\n",
    "    a = sns.boxplot(data=df.mask(df < 1.0), ax=axes[0])\n",
    "    a.set_title('Boxplot for each assignment')\n",
    "    a.set_ylim(1, 10)\n",
    "    sns.despine()\n",
    "    flatui = [\"#808080\", \"#FF0000\", \"#FFA500\", \"#FFFF00\", \"#008000\"]\n",
    "    sns.set_palette(flatui)\n",
    "    b = overviewdf.plot.bar(\n",
    "        stacked=True,\n",
    "        color=flatui,\n",
    "        ylim=(0, overviewdf.sum(axis=1).max()),\n",
    "        width=1.0,\n",
    "        legend='reverse',\n",
    "        ax=axes[1])\n",
    "    b.set_title(\n",
    "        'How many students have suifficient grades to pass after that assignment'\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(\n",
    "        loc='right',\n",
    "        bbox_to_anchor=(1.4, 0.8),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_validity():\n",
    "    canvas_grades = total_df(gradedict)\n",
    "    cronbach_df = cronbach_alpha_plot()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    a = sns.heatmap(\n",
    "        canvas_grades.corr(),\n",
    "        vmin=-1,\n",
    "        vmax=1.0,\n",
    "        annot=True,\n",
    "        linewidths=.5,\n",
    "        cmap=\"RdYlGn\",\n",
    "        ax=axes[0])\n",
    "    a.set_title(\"Correlations between assignments\")\n",
    "    a.set(ylabel='', xlabel='')\n",
    "\n",
    "    b = sns.barplot(\n",
    "        y=\"Assignment\",\n",
    "        x=\"Cronbachs Alpha\",\n",
    "        data=cronbach_df,\n",
    "        palette=map(color_ca_plot, cronbach_df[\"Cronbachs Alpha\"]),\n",
    "        ax=axes[1])\n",
    "    b.set_xlim(0, 1.0)\n",
    "    b.set(ylabel='', yticks=[])\n",
    "    b.set_title(\"Cronbachs Alpha for each assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def color_ca_plot(c):\n",
    "    pal = sns.color_palette(\"RdYlGn_r\", 6)\n",
    "    if c >= 0.8:\n",
    "        return pal[0]\n",
    "    elif c >= 0.6:\n",
    "        return pal[1]\n",
    "    else:\n",
    "        return pal[5]\n",
    "\n",
    "\n",
    "def cronbach_alpha_plot():\n",
    "    testlist = []\n",
    "    df = pd.pivot_table(\n",
    "        create_results_per_question(),\n",
    "        values='final_score',\n",
    "        index=['student_id'],\n",
    "        columns=['assignment', 'question_name'],\n",
    "        aggfunc=np.sum)\n",
    "\n",
    "    for assignment_id in sorted(set(df.columns.get_level_values(0))):\n",
    "        items = df[assignment_id].dropna(how='all').fillna(0)\n",
    "\n",
    "        # source: https://github.com/anthropedia/tci-stats/blob/master/tcistats/__init__.py\n",
    "        items_count = items.shape[1]\n",
    "        variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "        total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "\n",
    "        testlist.append((assignment_id, (items_count / float(items_count - 1) *\n",
    "                                         (1 - variance_sum / total_var))))\n",
    "\n",
    "    cronbach_df = pd.DataFrame(\n",
    "        testlist, columns=[\"Assignment\", \"Cronbachs Alpha\"])\n",
    "    return cronbach_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canvas_and_nbgrader():\n",
    "    canvas = set(assignment.name for assignment in course.get_assignments())\n",
    "    nbgrader = set(assignment for assignment in nbgrader_api.get_source_assignments())\n",
    "    return sorted(canvas & nbgrader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graded_submissions():\n",
    "    return [x['name'] for x in nbgrader_api.get_assignments() if x['num_submissions'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_student_ids(course):\n",
    "    return {student.id: student.sis_user_id for student in course.get_users()}\n",
    "\n",
    "\n",
    "def get_assignment_obj(course, assignment_name):\n",
    "    return {\n",
    "        assignment.name: assignment\n",
    "        for assignment in course.get_assignments()\n",
    "    }[assignment_name]"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
