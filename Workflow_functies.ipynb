{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nbgrader.apps import NbGraderAPI\n",
    "from traitlets.config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import nbgrader\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from canvasapi import Canvas\n",
    "from IPython.display import Javascript, Markdown, display\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive, widgets, Button, Layout\n",
    "from tqdm import tqdm, tqdm_notebook  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def assign(assignment_id):\n",
    "    !nbgrader assign {assignment_id} --create --force --IncludeHeaderFooter.header=source/header.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def download_files(assignment_id, course):\n",
    "    directory = 'downloaded/%s/archive/' % assignment_id\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get sis id's from students\n",
    "    student_dict = {\n",
    "        student.id: student.sis_user_id\n",
    "        for student in course.get_users()\n",
    "    }\n",
    "\n",
    "    # Get the Canvas assignment id\n",
    "    assignment = {\n",
    "        assignment.name: assignment\n",
    "        for assignment in course.get_assignments()\n",
    "    }[assignment_id]\n",
    "    submissions = assignment.get_submissions()\n",
    "\n",
    "    for submission in tqdm_notebook(submissions):\n",
    "        # Check if submission has attachments\n",
    "        if 'attachments' not in submission.attributes:\n",
    "            continue\n",
    "        # Download file and give correct name\n",
    "        student_id = student_dict[submission.user_id]\n",
    "        attachment = submission.attributes[\"attachments\"][0]\n",
    "        filename = str(student_id) + \"_\" + assignment_id + \".ipynb\"\n",
    "        urllib.request.urlretrieve(attachment['url'], directory + filename)\n",
    "        # Clear all notebooks of output to save memory\n",
    "        !nbstripout {directory + filename}\n",
    "    # Move the download files to submission folder\n",
    "    !nbgrader zip_collect {assignment_id} --force --log-level='INFO'\n",
    "\n",
    "    # Delete folders which aren't necessary\n",
    "    shutil.rmtree('downloaded/%s/archive/' % assignment_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def update_db(b):\n",
    "    # Check which students are already in nbgrader database\n",
    "    students_already_in_db = [\n",
    "        student.id for student in nbgrader_api.gradebook.students\n",
    "    ]\n",
    "\n",
    "    for student in tqdm_notebook(course.get_users(enrollment_type=['student'])):\n",
    "        first_name, last_name = student.name.split(' ', 1)\n",
    "        # Add students that are not yet in nbgrader database\n",
    "        if student.sis_user_id not in students_already_in_db:\n",
    "            nbgrader_api.gradebook.add_student(\n",
    "                str(student.sis_user_id),\n",
    "                first_name=first_name,\n",
    "                last_name=last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def autograde(assignment_id):\n",
    "    !nbgrader autograde {assignment_id} --create --force --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def plagiatcheck(assignment_id):\n",
    "    !jupyter nbconvert --to script downloaded/{assignment_id}/extracted/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/pyfiles / --log-level WARN\n",
    "    !jupyter nbconvert --to script release/{assignment_id}/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/base / --log-level WARN\n",
    "    shutil.rmtree('downloaded/%s/extracted/' % assignment_id)\n",
    "    directory = \"plagiaatcheck/%s/pyfiles/\" % assignment_id\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            targetfilename = file[:-3] + \"py\"\n",
    "            if targetfilename in os.listdir(directory):\n",
    "                os.remove(directory + targetfilename)\n",
    "            os.rename(directory + file, directory + targetfilename)\n",
    "    if not sys.platform.startswith('win'):\n",
    "        !compare50 plagiaatcheck/{assignment_id}/pyfiles/* -d plagiaatcheck/{assignment_id}/base/\n",
    "        #!compare50 pyfiles/* -d base/\n",
    "    else:\n",
    "        print(\"Oeps, voor compare50 heb je Linux of Mac nodig.\")\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<a class=\"btn btn-primary\" style=\"margin-top: 10px; text-decoration: none;\" href=\"plagiaatcheck/%s/\" target=\"_blank\">Open map met plagiaatresultaten</a>'\n",
    "            % assignment_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_feedback(student_id, assignment_id):\n",
    "    \"\"\"Given a student_id and assignment_id, creates a feedback file without the Hidden Tests\"\"\"\n",
    "    directory = 'feedback/%s/%s/' % (student_id, assignment_id)\n",
    "    soup = str(\n",
    "        BeautifulSoup(\n",
    "            open(\"%s%s.html\" % (directory, assignment_id), encoding='utf-8'),\n",
    "            \"html.parser\"))\n",
    "    css, html = soup.split('</head>', 1)\n",
    "    html = re.sub(\n",
    "        r'(<div class=\"output_subarea output_text output_error\">\\n<pre>\\n)(?:(?!<\\/div>)[\\w\\W])*(<span class=\"ansi-red-intense-fg ansi-bold\">[\\w\\W]*?<\\/pre>)',\n",
    "        r'\\1\\2', html)\n",
    "    html = re.sub(\n",
    "        r'<span class=\"c1\">### BEGIN HIDDEN TESTS<\\/span>[\\w\\W]*?<span class=\"c1\">### END HIDDEN TESTS<\\/span>',\n",
    "        '', html)\n",
    "    soup = css + '</head>' + html\n",
    "    targetdirectory = 'canvasfeedback/%s/%s/' % (student_id, assignment_id)\n",
    "    if not os.path.exists(targetdirectory):\n",
    "        os.makedirs(targetdirectory)\n",
    "    filename = \"%s%s.html\" % (targetdirectory, assignment_id)\n",
    "    Html_file = open(filename, \"w\", encoding=\"utf8\")\n",
    "    Html_file.write(soup)\n",
    "    Html_file.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def calculate_grade(score, min_grade, max_score):\n",
    "    \"\"\"Calculate grade for an assignment\"\"\"\n",
    "    return max(\n",
    "        1, min(\n",
    "            round(min_grade + (10 - min_grade) * score / max_score, 1), 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_canvas_grades():\n",
    "    \"\"\"Creates a dataframe with the grades for each person and each assignment\"\"\"\n",
    "    q = '''\n",
    "        SELECT\n",
    "        \n",
    "            submitted_assignment.student_id,\n",
    "            assignment.name AS assignment,\n",
    "            SUM(grade_cell.max_score) as max_score,\n",
    "            SUM(grade.auto_score) as auto_score,\n",
    "            SUM(grade.manual_score) as manual_score,\n",
    "            SUM(grade.extra_credit) as extra_credit\n",
    "            \n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "        GROUP BY submitted_assignment.student_id, assignment.name\n",
    "    '''\n",
    "\n",
    "    canvasdf = pd.read_sql_query(q, 'sqlite:///gradebook.db').fillna(0)\n",
    "    canvasdf['student_id'] = pd.to_numeric(canvasdf['student_id'])\n",
    "    return canvasdf\n",
    "\n",
    "\n",
    "def create_grades_per_assignment(assignment_name, canvasdf, gradedict):\n",
    "    canvasdf = canvasdf[canvasdf.assignment == assignment_name]\n",
    "    canvasdf[\"score\"] = canvasdf[\"auto_score\"] + canvasdf[\n",
    "        \"manual_score\"] + canvasdf[\"extra_credit\"]\n",
    "    max_score = gradedict[assignment_name][\n",
    "        \"max_score\"] if assignment_name in gradedict.keys(\n",
    "        ) else canvasdf['max_score'].max()\n",
    "    min_grade = gradedict[assignment_name][\n",
    "        \"min_grade\"] if assignment_name in gradedict.keys() else 0\n",
    "    canvasdf['grade'] = canvasdf[['score', 'assignment', 'max_score']].apply(\n",
    "        lambda row: calculate_grade(row[0], min_grade, max_score), axis=1)\n",
    "    canvasdf = canvasdf.pivot_table(\n",
    "        values='grade',\n",
    "        index='student_id',\n",
    "        columns='assignment',\n",
    "        aggfunc='first')\n",
    "    return canvasdf\n",
    "\n",
    "\n",
    "def total_df(gradedict):\n",
    "    canvasdf = create_canvas_grades()\n",
    "    canvasdf = pd.concat([\n",
    "        create_grades_per_assignment(x, canvasdf, gradedict)\n",
    "        for x in canvasdf.assignment.unique()\n",
    "    ],\n",
    "                         axis=1)\n",
    "    return canvasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def visualize_grades(assignment_id, gradedict):\n",
    "    \"\"\"Creates a plot of the grades from a specific assignment\"\"\"\n",
    "    canvasdf = create_canvas_grades()\n",
    "    grades = create_grades_per_assignment(assignment_id, canvasdf,\n",
    "                                          gradedict)[assignment_id]\n",
    "    # ignore grades equal to 1.0\n",
    "    grades = grades.where(grades >= 1.0).dropna()\n",
    "    print(\"The mean grade is {:.1f}\".format(grades.mean()))\n",
    "    print(\"The median grade is {}\".format(grades.median()))\n",
    "    print(\"Maximum van Cohen-Schotanus is {:.1f}\".format(\n",
    "        grades.nlargest(max(5, int(len(grades) * 0.05))).mean()))\n",
    "    print(\"Het percentage onvoldoendes is {:.1f}%. \".format(\n",
    "        100 * sum(grades < 5.5) / len(grades)))\n",
    "    if 100 * sum(grades < 5.5) / len(grades) > 30:\n",
    "        print(\n",
    "            \"Het percentage onvoldoendes is te hoog, voor meer informatie kijk op: {}\"\n",
    "            .format(\n",
    "                \"http://toetsing.uva.nl/toetscyclus/analyseren/tentamenanalyse/tentamenanalyse.html#anker-percentage-geslaagde-studenten\"\n",
    "            ))\n",
    "    fig = sns.distplot(\n",
    "        grades, kde_kws={'clip': (0.0, 10.0)}, bins=np.arange(1, 11, 1))\n",
    "    fig.set_xlim(1, 10)\n",
    "    fig.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "def p_value(df):\n",
    "    \"\"\"Creates a barchart of how many points people on average received for a question of a specific assignment\"\"\"\n",
    "    return df.groupby(\n",
    "        'question_name', sort=False)['final_score'].mean() / df.groupby(\n",
    "            'question_name', sort=False)['max_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_results_per_question():\n",
    "    q = '''\n",
    "        SELECT\n",
    "            submitted_assignment.student_id,\n",
    "            grade_cell.name AS question_name,\n",
    "            grade_cell.max_score,\n",
    "            grade.needs_manual_grade AS needs_grading,\n",
    "            grade.auto_score,\n",
    "            grade.manual_score,\n",
    "            grade.extra_credit,\n",
    "            assignment.name AS assignment\n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql_query(q, 'sqlite:///gradebook.db')\n",
    "    \n",
    "    df['final_score'] = np.where(\n",
    "        ~pd.isnull(df['manual_score']), df['manual_score'],\n",
    "        df['auto_score']) + df['extra_credit'].fillna(0)\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_visualizations(assignment_id):\n",
    "    df = create_results_per_question()\n",
    "    df = df.loc[df['assignment'] == assignment_id]\n",
    "    p_df = p_value(df)\n",
    "    testdf = create_rir(df)\n",
    "    temp = pd.concat([p_df, testdf], axis=1)\n",
    "    temp = temp.reindex(list(p_df.index))\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns = [\"Question\", \"P value\", \"Rir value\", \"positive\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 7), sharey=True)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.suptitle('P value and Rir value per question')\n",
    "    sns.barplot(\n",
    "        x=\"P value\", y=\"Question\", data=temp, color='b', ax=axes[0]).set_xlim(\n",
    "            0, 1.0)\n",
    "    sns.barplot(\n",
    "        x=\"Rir value\",\n",
    "        y=\"Question\",\n",
    "        data=temp,\n",
    "        ax=axes[1],\n",
    "        palette=temp[\"positive\"]).set_xlim(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def color_ca_plot(c):\n",
    "    pal = sns.color_palette(\"RdYlGn_r\", 6)\n",
    "    if c >= 0.8:\n",
    "        return pal[0]\n",
    "    elif c >= 0.6:\n",
    "        return pal[1]\n",
    "    else:\n",
    "        return pal[5]\n",
    "\n",
    "\n",
    "def cronbach_alpha_plot():\n",
    "    testlist = []\n",
    "    df = pd.pivot_table(\n",
    "        create_results_per_question(),\n",
    "        values='final_score',\n",
    "        index=['student_id'],\n",
    "        columns=['assignment', 'question_name'],\n",
    "        aggfunc=np.sum)\n",
    "\n",
    "    for assignment_id in sorted(set(df.columns.get_level_values(0))):\n",
    "        items = df[assignment_id].dropna(how='any')\n",
    "\n",
    "        # source: https://github.com/anthropedia/tci-stats/blob/master/tcistats/__init__.py\n",
    "        items_count = items.shape[1]\n",
    "        variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "        total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "\n",
    "        testlist.append((assignment_id, (items_count / float(items_count - 1) *\n",
    "                                         (1 - variance_sum / total_var))))\n",
    "    \n",
    "    assignment_list, ca = list(zip(*testlist))\n",
    "    return ca, testlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['Rir-waarde'] <= 0:\n",
    "        return 'r'\n",
    "    elif row['Rir-waarde'] <= 0.25:\n",
    "        return 'y'\n",
    "    else:\n",
    "        return 'g'\n",
    "\n",
    "\n",
    "def create_rir(df):\n",
    "    testdict = {}\n",
    "\n",
    "    if len(df[\"student_id\"].unique()) <= 50:\n",
    "        print(\"Norm of 50 students not reached to be meaningful\")\n",
    "\n",
    "    df[\"total_score_item\"] = df[\"extra_credit\"] + df[\"auto_score\"] + df[\n",
    "        \"manual_score\"]\n",
    "    df['student_score-item'] = df['total_score_item'].groupby(\n",
    "        df['student_id']).transform('sum') - df['total_score_item']\n",
    "    for question in sorted(set(df[\"question_name\"].values)):\n",
    "        temp_df = df.loc[df['question_name'] == question]\n",
    "        testdict[question] = temp_df[[\n",
    "            \"total_score_item\", \"student_score-item\"\n",
    "        ]].corr().iloc[1, 0]\n",
    "    testdf = pd.DataFrame.from_dict(\n",
    "        testdict, orient='index', columns=[\"Rir-waarde\"])\n",
    "    testdf['positive'] = testdf.apply(f, axis=1)\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def upload_to_canvas(assignment_name, message, feedback=False):\n",
    "    print(feedback,assignment_name)\n",
    "    if feedback:\n",
    "        !nbgrader feedback --quiet --force --assignment={assignment_name}\n",
    "    # Haal de laatste cijfers uit gradebook\n",
    "    canvasdf = total_df(gradedict)\n",
    "    student_dict = {\n",
    "        student.id: student.sis_user_id\n",
    "        for student in course.get_users()\n",
    "    }\n",
    "    assignment = [x for x in course.get_assignments() if x.name==assignment_name][0]\n",
    "    # loop over alle submissions voor een assignment, alleen als er attachments zijn\n",
    "    for submission in tqdm_notebook(\n",
    "            assignment.get_submissions(), desc='Submissions', leave=False):\n",
    "        try:\n",
    "            student_id = student_dict[submission.user_id]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if int(student_id) not in list(canvasdf.index.values):\n",
    "            continue\n",
    "        grade = canvasdf.at[int(student_id), assignment_name]\n",
    "        if np.isnan(grade):\n",
    "            continue\n",
    "        # alleen de cijfers veranderen als die op canvas lager zijn of niet bestaan\n",
    "        if submission.attributes['score'] == None:\n",
    "            pass\n",
    "        elif submission.attributes['score'] == grade or submission.attributes['score'] ==0:\n",
    "            continue\n",
    "        print(grade, student_id)\n",
    "        if feedback:\n",
    "            feedbackfile = create_feedback(student_id,\n",
    "                                       assignment_name)\n",
    "            #submission.upload_comment(feedbackfile)\n",
    "        #submission.edit(submission={'posted_grade': str(grade)}, comment={'text_comment':message})\n",
    "        \n",
    "    # feedbackfile verwijderen, om ruimte te besparen.\n",
    "    if 'canvasfeedback' in os.listdir():\n",
    "        shutil.rmtree('canvasfeedback/', ignore_errors=True)\n",
    "    if 'feedback' in os.listdir():\n",
    "        shutil.rmtree('feedback/', ignore_errors=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_button = Button(\n",
    "    description=\"Update the students in the database\",\n",
    "    layout=Layout(width='300px'))\n",
    "db_button.on_click(update_db)\n",
    "\n",
    "interact_assign = interact_manual.options(\n",
    "    manual_name=\"Assign de assignment in de database\")\n",
    "\n",
    "\n",
    "canvas_button = interact_manual.options(\n",
    "    manual_name=\"Cijfers naar Canvas jwz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overview(df,sequence):\n",
    "    df = df.fillna(0)\n",
    "    testlist = []\n",
    "    l = [\n",
    "        x for x in sequence if x in df.columns\n",
    "    ]\n",
    "\n",
    "    for n, c in enumerate(l):\n",
    "        kolommen_assignments = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"AssignmentWeek\")])\n",
    "        kolommen_deeltoets = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"Deeltoets\")])\n",
    "        if kolommen_deeltoets == set():\n",
    "            voldoende_deeltoets = pd.Series(\n",
    "                [True for x in range(len(df.index))], index=df.index)\n",
    "        else:\n",
    "            voldoende_deeltoets = df[kolommen_deeltoets].mean(axis=1) >= 5.5\n",
    "        voldoende_assignments = df[kolommen_assignments].mean(axis=1) >= 5.5\n",
    "        testlist.append(\n",
    "            [c] + [(x & y).sum()\n",
    "                   for x in [~voldoende_deeltoets, voldoende_deeltoets]\n",
    "                   for y in [~voldoende_assignments, voldoende_assignments]])\n",
    "\n",
    "    testdf = pd.DataFrame(\n",
    "        testlist,\n",
    "        columns=[\n",
    "            \"Assignment Name\", \"Onvoldoende voor beide onderdelen\",\n",
    "            \"Onvoldoende voor deeltoets\", \"Onvoldoende voor assignments\",\n",
    "            \"Voldoende voor beide onderdelen\"\n",
    "        ]).set_index(\"Assignment Name\")\n",
    "    return testdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_overview(gradedict,sequence):\n",
    "    df = total_df(gradedict)\n",
    "    overviewdf = create_overview(df,sequence)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.suptitle('IETS')\n",
    "    sns.boxplot(data=df.mask(df < 1.0), ax=axes[0]).set_ylim(1,10)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.title(\n",
    "         'How many students have suifficient grades to pass after that assignment',\n",
    "         color='black')\n",
    "    overviewdf.plot.bar(\n",
    "        stacked=True,\n",
    "        color=sns.color_palette(\"RdYlGn\",5),\n",
    "        ylim=(0, overviewdf.sum(axis=1).max()),\n",
    "        width=1.0,\n",
    "        legend='reverse',\n",
    "        ax=axes[1])\n",
    "#     plt.legend(\n",
    "#         loc='upper center',\n",
    "#         bbox_to_anchor=(0.5, -0.15),\n",
    "#         fancybox=True,\n",
    "#         shadow=True,\n",
    "#         ncol=len(testdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_validity():\n",
    "    canvas_grades = total_df(gradedict)\n",
    "    ca, testlist = cronbach_alpha_plot()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    sns.heatmap(\n",
    "        canvas_grades.mask(canvas_grades < 1.0).corr(),\n",
    "        vmin=-1,\n",
    "        vmax=1.0,\n",
    "        annot=True,\n",
    "        linewidths=.5,\n",
    "        cmap=\"RdYlGn\",\n",
    "        ax = axes[0])\n",
    "    \n",
    "\n",
    "    sns.barplot(x=0,y=1,\n",
    "        data=pd.DataFrame(testlist), palette=map(color_ca_plot, ca), ax=axes[1]).set_ylim(\n",
    "            0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canvas_and_nbgrader():\n",
    "    canvas = set(assignment.name for assignment in course.get_assignments())\n",
    "    nbgrader = set(assignment for assignment in nbgrader_api.get_source_assignments())\n",
    "    return sorted(canvas & nbgrader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graded_submissions():\n",
    "    return list(create_canvas_grades().assignment.unique())"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
