{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "from nbgrader.apps import NbGraderAPI\n",
    "from traitlets.config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import nbgrader\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from canvasapi import Canvas\n",
    "from IPython.display import Javascript, Markdown, display\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive, widgets, Button, Layout\n",
    "from tqdm import tqdm, tqdm_notebook  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def assign(assignment_id):\n",
    "    !nbgrader assign {assignment_id} --create --force --IncludeHeaderFooter.header=source/header.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def download_files(assignment_id, course):\n",
    "    directory = 'downloaded/%s/archive/' % assignment_id\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get sis id's from students\n",
    "    student_dict = {\n",
    "        student.id: student.sis_user_id\n",
    "        for student in course.get_users()\n",
    "    }\n",
    "\n",
    "    # Get the Canvas assignment id\n",
    "    assignment = {\n",
    "        assignment.name: assignment\n",
    "        for assignment in course.get_assignments()\n",
    "    }[assignment_id]\n",
    "    submissions = assignment.get_submissions()\n",
    "\n",
    "    for submission in tqdm_notebook(submissions):\n",
    "        # Check if submission has attachments\n",
    "        if 'attachments' not in submission.attributes:\n",
    "            continue\n",
    "        # Download file and give correct name\n",
    "        student_id = student_dict[submission.user_id]\n",
    "        attachment = submission.attributes[\"attachments\"][0]\n",
    "        filename = str(student_id) + \"_\" + assignment_id + \".ipynb\"\n",
    "        urllib.request.urlretrieve(attachment['url'], directory + filename)\n",
    "        # Clear all notebooks of output to save memory\n",
    "        !nbstripout {directory + filename}\n",
    "    # Move the download files to submission folder\n",
    "    !nbgrader zip_collect {assignment_id} --force --log-level='INFO'\n",
    "\n",
    "    # Delete folders which aren't necessary\n",
    "    shutil.rmtree('downloaded/%s/archive/' % assignment_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def update_db(b):\n",
    "    # Check which students are already in nbgrader database\n",
    "    students_already_in_db = [\n",
    "        student.id for student in nbgrader_api.gradebook.students\n",
    "    ]\n",
    "\n",
    "    for student in tqdm_notebook(course.get_users(enrollment_type=['student'])):\n",
    "        first_name, last_name = student.name.split(' ', 1)\n",
    "        # Add students that are not yet in nbgrader database\n",
    "        if student.sis_user_id not in students_already_in_db:\n",
    "            nbgrader_api.gradebook.add_student(\n",
    "                str(student.sis_user_id),\n",
    "                first_name=first_name,\n",
    "                last_name=last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def autograde(assignment_id):\n",
    "    !nbgrader autograde {assignment_id} --create --force --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def plagiatcheck(assignment_id):\n",
    "    !jupyter nbconvert --to script downloaded/{assignment_id}/extracted/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/pyfiles / --log-level WARN\n",
    "    !jupyter nbconvert --to script release/{assignment_id}/*.ipynb --output-dir=plagiaatcheck/{assignment_id}/base / --log-level WARN\n",
    "    shutil.rmtree('downloaded/%s/extracted/' % assignment_id)\n",
    "    directory = \"plagiaatcheck/%s/pyfiles/\" % assignment_id\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            targetfilename = file[:-3] + \"py\"\n",
    "            if targetfilename in os.listdir(directory):\n",
    "                os.remove(directory + targetfilename)\n",
    "            os.rename(directory + file, directory + targetfilename)\n",
    "    if not sys.platform.startswith('win'):\n",
    "        !compare50 plagiaatcheck/{assignment_id}/pyfiles/* -d plagiaatcheck/{assignment_id}/base/\n",
    "        #!compare50 pyfiles/* -d base/\n",
    "    else:\n",
    "        print(\"Oeps, voor compare50 heb je Linux of Mac nodig.\")\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<a class=\"btn btn-primary\" style=\"margin-top: 10px; text-decoration: none;\" href=\"plagiaatcheck/%s/\" target=\"_blank\">Open map met plagiaatresultaten</a>'\n",
    "            % assignment_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_feedback(student_id, assignment_id):\n",
    "    \"\"\"Given a student_id and assignment_id, creates a feedback file without the Hidden Tests\"\"\"\n",
    "    directory = 'feedback/%s/%s/' % (student_id, assignment_id)\n",
    "    soup = str(\n",
    "        BeautifulSoup(\n",
    "            open(\"%s%s.html\" % (directory, assignment_id), encoding='utf-8'),\n",
    "            \"html.parser\"))\n",
    "    css, html = soup.split('</head>', 1)\n",
    "    html = re.sub(\n",
    "        r'(<div class=\"output_subarea output_text output_error\">\\n<pre>\\n)(?:(?!<\\/div>)[\\w\\W])*(<span class=\"ansi-red-intense-fg ansi-bold\">[\\w\\W]*?<\\/pre>)',\n",
    "        r'\\1\\2', html)\n",
    "    html = re.sub(\n",
    "        r'<span class=\"c1\">### BEGIN HIDDEN TESTS<\\/span>[\\w\\W]*?<span class=\"c1\">### END HIDDEN TESTS<\\/span>',\n",
    "        '', html)\n",
    "    soup = css + '</head>' + html\n",
    "    targetdirectory = 'canvasfeedback/%s/%s/' % (student_id, assignment_id)\n",
    "    if not os.path.exists(targetdirectory):\n",
    "        os.makedirs(targetdirectory)\n",
    "    filename = \"%s%s.html\" % (targetdirectory, assignment_id)\n",
    "    Html_file = open(filename, \"w\", encoding=\"utf8\")\n",
    "    Html_file.write(soup)\n",
    "    Html_file.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def calculate_grade(score, assignment, max_score, gradedict):\n",
    "    \"\"\"Calculate grade for an assignment\"\"\"\n",
    "    max_score = gradedict[assignment][\n",
    "        \"max_score\"] if assignment in gradedict.keys() else max_score\n",
    "    min_grade = gradedict[assignment][\n",
    "        \"min_grade\"] if assignment in gradedict.keys() else 0\n",
    "    return max(1, min(\n",
    "        round(min_grade + (10 - min_grade) * score / max_score, 1), 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_canvas_grades(gradedict):\n",
    "    \"\"\"Creates a dataframe with the grades for each person and each assignment\"\"\"\n",
    "    q = '''\n",
    "        SELECT\n",
    "        \n",
    "            submitted_assignment.student_id,\n",
    "            assignment.name AS assignment,\n",
    "            SUM(grade_cell.max_score) as max_score,\n",
    "            SUM(grade.auto_score) as auto_score,\n",
    "            SUM(grade.manual_score) as manual_score,\n",
    "            SUM(grade.extra_credit) as extra_credit\n",
    "            \n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "        GROUP BY submitted_assignment.student_id, assignment.name\n",
    "    '''\n",
    "\n",
    "    canvasdf = pd.read_sql_query(q, 'sqlite:///gradebook.db').fillna(0)\n",
    "    canvasdf['student_id'] = pd.to_numeric(canvasdf['student_id'])\n",
    "    canvasdf[\"score\"] = canvasdf[\"auto_score\"] + canvasdf[\n",
    "        \"manual_score\"] + canvasdf[\"extra_credit\"]\n",
    "    canvasdf['grade'] = canvasdf[['score', 'assignment', 'max_score']].apply(\n",
    "        lambda row: calculate_grade(row[0], row[1], row[2], gradedict), axis=1)\n",
    "    canvasdf = canvasdf.pivot_table(\n",
    "        values='grade',\n",
    "        index='student_id',\n",
    "        columns='assignment',\n",
    "        aggfunc='first')\n",
    "    canvasdf.to_csv('canvas.csv')\n",
    "    return canvasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_results_per_question():\n",
    "    q = '''\n",
    "        SELECT\n",
    "            submitted_assignment.student_id,\n",
    "            grade_cell.name AS question_name,\n",
    "            grade_cell.max_score,\n",
    "            grade.needs_manual_grade AS needs_grading,\n",
    "            grade.auto_score,\n",
    "            grade.manual_score,\n",
    "            grade.extra_credit,\n",
    "            assignment.name AS assignment\n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql_query(q, 'sqlite:///gradebook.db')\n",
    "    df['final_score'] = np.where(\n",
    "        ~pd.isnull(df['manual_score']), df['manual_score'],\n",
    "        df['auto_score']) + df['extra_credit'].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def visualize_grades(assignment_id, canvas_grades):\n",
    "    \"\"\"Creates a plot of the grades from a specific assignment\"\"\"\n",
    "    grades = canvas_grades[assignment_id]\n",
    "    # ignore grades equal to 1.0\n",
    "    grades = grades.where(grades >= 1.0).dropna()\n",
    "    print(\"The mean grade is {:.1f}\".format(grades.mean()))\n",
    "    print(\"The median grade is {}\".format(grades.median()))\n",
    "    print(\"Maximum van Cohen-Schotanus is {}\".format(grades.nlargest(max(5,int(len(grades)*0.05))).mean()))\n",
    "    print(\"Het percentage onvoldoendes is {:.1f}%. \".format(100* sum(grades < 5.5)/len(grades)))\n",
    "    if 100* sum(grades < 5.5)/len(grades) > 30:\n",
    "        print(\"Het percentage onvoldoendes is te hoog, voor meer informatie kijk op: {}\".format(\"http://toetsing.uva.nl/toetscyclus/analyseren/tentamenanalyse/tentamenanalyse.html#anker-percentage-geslaagde-studenten\"))\n",
    "    fig = sns.distplot(grades, kde_kws={'clip': (0.0, 10.0)}, bins=np.arange(1, 11, 1))\n",
    "    fig.set_xlim(1, 10)\n",
    "    fig.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "def visualize_questions(assignment_id, df):\n",
    "    \"\"\"Creates a barchart of how many points people on average received for a question of a specific assignment\"\"\"\n",
    "    q = df.loc[df['assignment'] == assignment_id]\n",
    "    q = q.fillna(0)\n",
    "    q = q.groupby('question_name')['final_score'].mean() / q.groupby(\n",
    "        'question_name')['max_score'].mean()\n",
    "    q.plot(kind='barh', figsize=(15, 8), xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def color_ca_plot(c):\n",
    "    pal = sns.color_palette(\"RdYlGn_r\", 6)\n",
    "    if c >= 0.9:\n",
    "        return pal[0]\n",
    "    elif c >= 0.8:\n",
    "        return pal[1]\n",
    "    elif c >= 0.7:\n",
    "        return pal[2]\n",
    "    elif c >= 0.6:\n",
    "        return pal[3]\n",
    "    elif c >= 0.5:\n",
    "        return pal[4]\n",
    "    else:\n",
    "        return pal[5]\n",
    "\n",
    "\n",
    "def cronbach_alpha_plot():\n",
    "    testlist = []\n",
    "    df = pd.pivot_table(\n",
    "        create_results_per_question(),\n",
    "        values='final_score',\n",
    "        index=['student_id'],\n",
    "        columns=['assignment', 'question_name'],\n",
    "        aggfunc=np.sum)\n",
    "    for assignment_id in sorted(set(df.columns.get_level_values(0))):\n",
    "        items = df[assignment_id].dropna(how='any')\n",
    "\n",
    "        # source: https://github.com/anthropedia/tci-stats/blob/master/tcistats/__init__.py\n",
    "        items_count = items.shape[1]\n",
    "        variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "        total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "\n",
    "        testlist.append((assignment_id, (items_count / float(items_count - 1) *\n",
    "                                         (1 - variance_sum / total_var))))\n",
    "    \n",
    "    assignment_list, ca = list(zip(*testlist))\n",
    "    sns.set_style('ticks')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "    sns.barplot(x=0,y=1,\n",
    "        data=pd.DataFrame(testlist), palette=map(color_ca_plot, ca)).set_ylim(\n",
    "            0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['Rir-waarde'] <= 0:\n",
    "        return 'r'\n",
    "    elif row['Rir-waarde'] <= 0.25:\n",
    "        return 'y'\n",
    "    else:\n",
    "        return 'g'\n",
    "\n",
    "\n",
    "def create_rir(assignment_id):\n",
    "    q = '''\n",
    "        SELECT\n",
    "            submitted_assignment.student_id,\n",
    "            grade_cell.name AS question_name,\n",
    "            grade_cell.max_score,\n",
    "            grade.needs_manual_grade AS needs_grading,\n",
    "            grade.auto_score,\n",
    "            grade.manual_score,\n",
    "            grade.extra_credit,\n",
    "            assignment.name AS assignment\n",
    "        FROM grade\n",
    "            INNER JOIN submitted_notebook ON submitted_notebook.id = grade.notebook_id\n",
    "            INNER JOIN submitted_assignment ON submitted_assignment.id = submitted_notebook.assignment_id\n",
    "            INNER JOIN grade_cell ON grade_cell.id = grade.cell_id\n",
    "            INNER JOIN assignment ON submitted_assignment.assignment_id = assignment.id\n",
    "    '''\n",
    "    testdict = {}\n",
    "    df = pd.read_sql_query(q, 'sqlite:///gradebook.db').fillna(0)\n",
    "    df = df.loc[df['assignment'] == assignment_id]\n",
    "\n",
    "    if len(df[\"student_id\"].unique()) <= 50:\n",
    "        print(\"Norm of 50 students not reached to be meaningful\")\n",
    "\n",
    "    df[\"total_score_item\"] = df[\"extra_credit\"] + df[\"auto_score\"] + df[\n",
    "        \"manual_score\"]\n",
    "    df['student_score-item'] = df['total_score_item'].groupby(\n",
    "        df['student_id']).transform('sum') - df['total_score_item']\n",
    "    for question in sorted(set(df[\"question_name\"].values)):\n",
    "        temp_df = df.loc[df['question_name'] == question]\n",
    "        testdict[question] = temp_df[[\n",
    "            \"total_score_item\", \"student_score-item\"\n",
    "        ]].corr().iloc[1, 0]\n",
    "    testdf = pd.DataFrame.from_dict(\n",
    "        testdict, orient='index', columns=[\"Rir-waarde\"])\n",
    "    testdf['positive'] = testdf.apply(f, axis=1)\n",
    "\n",
    "    ax = testdf.plot(kind='barh', color=[testdf.positive])\n",
    "    ax.set_xlim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def upload_to_canvas(b):\n",
    "    # Haal de laatste cijfers uit gradebook\n",
    "    canvasdf = create_canvas_grades(gradedict)\n",
    "    if 'student_dict' not in globals():\n",
    "        student_dict = {\n",
    "            student.id: student.sis_user_id\n",
    "            for student in course.get_users()\n",
    "        }\n",
    "\n",
    "    # loop over alle assignments heen, selecteer alleen de nbgraderopdrachten\n",
    "    for assignment in tqdm_notebook(course.get_assignments(), desc='Assignments'):\n",
    "\n",
    "        # converteer de canvas-assignment naam naar de nbgrader naam\n",
    "        assignment_name_nbgrader = assignment.name\n",
    "        if assignment_name_nbgrader not in list(canvasdf.columns):\n",
    "            continue\n",
    "        # loop over alle submissions voor een assignment, alleen als er attachments zijn\n",
    "        for submission in tqdm_notebook(\n",
    "                assignment.get_submissions(), desc='Submissions', leave=False):\n",
    "            try:\n",
    "                student_id = student_dict[submission.user_id]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if int(student_id) not in list(canvasdf.index.values):\n",
    "                continue\n",
    "            grade = canvasdf.at[int(student_id), assignment_name_nbgrader]\n",
    "            if np.isnan(grade):\n",
    "                continue\n",
    "            # alleen de cijfers veranderen als die op canvas lager zijn of niet bestaan\n",
    "            if submission.attributes['score'] == None:\n",
    "                pass\n",
    "            elif submission.attributes['score'] >= grade:\n",
    "                continue\n",
    "\n",
    "                # creeer feedbackfiles als die nog niet gemaakt zijn\n",
    "            if 'feedback' not in os.listdir():\n",
    "                !nbgrader feedback --quiet --force --assignment={assignment_name_nbgrader}\n",
    "            feedbackfile = create_feedback(student_id,\n",
    "                                           assignment_name_nbgrader)\n",
    "            submission.edit(submission={'posted_grade': str(grade)}, comment={'text_comment':\n",
    "                    message})\n",
    "            submission.upload_comment(feedbackfile)\n",
    "        # feedbackfile verwijderen, om ruimte te besparen.\n",
    "        if 'canvasfeedback' in os.listdir():\n",
    "            shutil.rmtree('canvasfeedback/', ignore_errors=True)\n",
    "        if 'feedback' in os.listdir():\n",
    "            shutil.rmtree('feedback/', ignore_errors=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_button = Button(\n",
    "    description=\"Update the students in the database\",\n",
    "    layout=Layout(width='300px'))\n",
    "db_button.on_click(update_db)\n",
    "\n",
    "interact_assign = interact_manual.options(\n",
    "    manual_name=\"Assign de assignment in de database\")\n",
    "\n",
    "\n",
    "canvas_button = Button(\n",
    "    description=\"Upload feedback and grades to Canvas\",\n",
    "    layout=Layout(width='300px'))\n",
    "\n",
    "canvas_button.on_click(upload_to_canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overview(gradedict):\n",
    "    df = create_canvas_grades(gradedict)\n",
    "    df = df.fillna(0)\n",
    "    testlist = []\n",
    "    l = [\n",
    "        x for x in [\n",
    "            \"AssignmentWeek1\", \"AssignmentWeek2\", \"AssignmentWeek3\",\n",
    "            \"Deeltoets1\", \"AssignmentWeek5\", \"AssignmentWeek6\",\n",
    "            \"AssignmentWeek7\", \"Deeltoets2\"\n",
    "        ] if x in df.columns\n",
    "    ]\n",
    "\n",
    "    for n, c in enumerate(l):\n",
    "        kolommen_assignments = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"AssignmentWeek\")])\n",
    "        kolommen_deeltoets = set(\n",
    "            [x for x in l[:n + 1] if x.startswith(\"Deeltoets\")])\n",
    "        if kolommen_deeltoets == set():\n",
    "            voldoende_deeltoets = pd.Series(\n",
    "                [True for x in range(len(df.index))], index=df.index)\n",
    "        else:\n",
    "            voldoende_deeltoets = df[kolommen_deeltoets].mean(axis=1) >= 5.5\n",
    "        voldoende_assignments = df[kolommen_assignments].mean(axis=1) >= 5.5\n",
    "        testlist.append(\n",
    "            [c] + [(x & y).sum()\n",
    "                   for x in [~voldoende_deeltoets, voldoende_deeltoets]\n",
    "                   for y in [~voldoende_assignments, voldoende_assignments]])\n",
    "\n",
    "    testdf = pd.DataFrame(\n",
    "        testlist,\n",
    "        columns=[\n",
    "            \"Assignment Name\", \"Onvoldoende voor beide onderdelen\",\n",
    "            \"Onvoldoende voor deeltoets\", \"Onvoldoende voor assignments\",\n",
    "            \"Voldoende voor beide onderdelen\"\n",
    "        ]).set_index(\"Assignment Name\")\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.title(\n",
    "        'Showing what percentage of students have suifficient grades to pass after that assignment',\n",
    "        color='black')\n",
    "    testdf.plot.bar(\n",
    "        stacked=True,\n",
    "        color=[(1, 0, 0), (1, 0.65, 0), (1, 1, 0), (0, 0.5, 0)],\n",
    "        ylim=(0, testdf.sum(axis=1).max()),\n",
    "        legend='reverse',\n",
    "        ax=f.gca(),\n",
    "        figsize=(10, 5))\n",
    "    plt.legend(\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.15),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=len(testdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlations():\n",
    "    sns.heatmap(\n",
    "        canvas_grades.mask(canvas_grades < 1.0).corr(),\n",
    "        vmin=0,\n",
    "        vmax=1.0,\n",
    "        annot=True,\n",
    "        linewidths=.5,\n",
    "        cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assignments_plot():\n",
    "    df = create_canvas_grades(gradedict)\n",
    "    sns.set_style('ticks')\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ylim(1, 10)\n",
    "    fig.set_size_inches(13, 8)\n",
    "    sns.boxplot(data=df.mask(df < 1.0), ax=ax)\n",
    "    sns.despine()"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
